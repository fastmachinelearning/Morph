================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2025-01-04T13:47:41.127Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
data/
  BraggnnDataset.py
  DeepsetsDataset.py
  get_dataset.py
examples/
  BraggNN/
    bragg_model_example_configs.yaml
    braggnn_search_space.yaml
    Global_search_Bragg.py
  DeepSets/
    deepsets_model_example_configs.yaml
    deepsets_search_space.yaml
    Global_search_DeepSets.py
models/
  blocks.py
utils/
  bops.py
  metrics.py
  processor.py
.gitignore
.pre-commit-config.yaml
global_search.py
local_search.py
README.md
requirements.txt

================================================================
Repository Files
================================================================

================
File: data/BraggnnDataset.py
================
import logging
import random

import os
import h5py
import numpy as np
import torch
from skimage import measure
from skimage.feature import peak_local_max
from torch.utils.data import DataLoader, Dataset


def clean_patch(p, center):
    w, h = p.shape
    cc = measure.label(p > 0)
    if cc.max() == 1:
        return p

    # logging.warn(f"{cc.max()} peaks located in a patch")
    lmin = np.inf
    cc_lmin = None
    for _c in range(1, cc.max() + 1):
        lmax = peak_local_max(p * (cc == _c), min_distance=1)
        if lmax.shape[0] == 0:
            continue  # single pixel component
        lc = lmax.mean(axis=0)
        dist = ((lc - center) ** 2).sum()
        if dist < lmin:
            cc_lmin = _c
            lmin = dist
    return p * (cc == cc_lmin)


class BraggNNDataset(Dataset):
    def __init__(self,data_folder, psz=11, rnd_shift=0, use="train", train_frac=0.8, test_frac=0.1):
        self.psz = psz
        self.rnd_shift = rnd_shift

        # peak_datapath= data_folder + f"peaks-exp4train-psz{psz}.hdf5"
        peak_datapath = os.path.join(data_folder, f"peaks-exp4train-psz{psz}.hdf5")
        with h5py.File(peak_datapath, "r") as h5fd:
        # with h5py.File("./data/peaks-exp4train-psz%d.hdf5" % psz, "r") as h5fd:
            total_samples = h5fd["peak_fidx"].shape[0]
            train_end = int(train_frac * total_samples)
            test_start = int((1 - test_frac) * total_samples)

            if use == "train":
                sti, edi = 0, train_end
            elif use == "validation":
                sti, edi = train_end, test_start
            elif use == "test":
                sti, edi = test_start, None
            else:
                logging.error(
                    f"Unsupported use: {use}. This class should be used for building training, validation, or test set"
                )

            mask = h5fd["npeaks"][sti:edi] == 1  # use only single-peak patches
            mask = mask & ((h5fd["deviations"][sti:edi] >= 0) & (h5fd["deviations"][sti:edi] < 1))

            self.peak_fidx = h5fd["peak_fidx"][sti:edi][mask]
            self.peak_row = h5fd["peak_row"][sti:edi][mask]
            self.peak_col = h5fd["peak_col"][sti:edi][mask]

        self.fidx_base = self.peak_fidx.min()
        # only loaded frames that will be used
        frames_datapath = os.path.join(data_folder, "frames-exp4train.hdf5")
        # with h5py.File("./data/frames-exp4train.hdf5", "r") as h5fd:
        with h5py.File(frames_datapath, "r") as h5fd:
            self.frames = h5fd["frames"][self.peak_fidx.min() : self.peak_fidx.max() + 1]

        self.len = self.peak_fidx.shape[0]

    def __getitem__(self, idx):
        _frame = self.frames[self.peak_fidx[idx] - self.fidx_base]
        if self.rnd_shift > 0:
            row_shift = np.random.randint(-self.rnd_shift, self.rnd_shift + 1)
            col_shift = np.random.randint(-self.rnd_shift, self.rnd_shift + 1)
        else:
            row_shift, col_shift = 0, 0
        prow_rnd = int(self.peak_row[idx]) + row_shift
        pcol_rnd = int(self.peak_col[idx]) + col_shift

        row_base = max(0, prow_rnd - self.psz // 2)
        col_base = max(0, pcol_rnd - self.psz // 2)

        crop_img = _frame[
            row_base : (prow_rnd + self.psz // 2 + self.psz % 2), col_base : (pcol_rnd + self.psz // 2 + self.psz % 2)
        ]
        # if((crop_img > 0).sum() == 1): continue # ignore single non-zero peak
        if crop_img.size != self.psz**2:
            c_pad_l = (self.psz - crop_img.shape[1]) // 2
            c_pad_r = self.psz - c_pad_l - crop_img.shape[1]

            r_pad_t = (self.psz - crop_img.shape[0]) // 2
            r_pad_b = self.psz - r_pad_t - crop_img.shape[0]

            logging.warn(f"sample {idx} touched edge when crop the patch: {crop_img.shape}")
            crop_img = np.pad(crop_img, ((r_pad_t, r_pad_b), (c_pad_l, c_pad_r)), mode="constant")
        else:
            c_pad_l, r_pad_t = 0, 0

        _center = np.array([self.peak_row[idx] - row_base + r_pad_t, self.peak_col[idx] - col_base + c_pad_l])
        crop_img = clean_patch(crop_img, _center)
        if crop_img.max() != crop_img.min():
            _min, _max = crop_img.min().astype(np.float32), crop_img.max().astype(np.float32)
            feature = (crop_img - _min) / (_max - _min)
        else:
            logging.warn("sample %d has unique intensity sum of %d" % (idx, crop_img.sum()))
            feature = crop_img

        px = (self.peak_col[idx] - col_base + c_pad_l) / self.psz
        py = (self.peak_row[idx] - row_base + r_pad_t) / self.psz

        return feature[np.newaxis], np.array([px, py]).astype(np.float32)

    def __len__(self):
        return self.len


def setup_data_loaders_braggnn(batch_size, IMG_SIZE, aug=0, num_workers=4, pin_memory=False, prefetch_factor=2, data_folder= "./data/"):
    ds_train = BraggNNDataset(data_folder=data_folder, psz=IMG_SIZE, rnd_shift=aug, use="train")
    dl_train = DataLoader(
        ds_train,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        drop_last=True,
        pin_memory=pin_memory,
    )
    # TODO: Change prefetch_factor back to 2 and pin_memory to true

    ds_valid = BraggNNDataset(data_folder=data_folder, psz=IMG_SIZE, rnd_shift=0, use="validation")
    dl_valid = DataLoader(
        ds_valid,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        drop_last=False,
        pin_memory=pin_memory,
    )

    ds_test = BraggNNDataset(data_folder=data_folder, psz=IMG_SIZE, rnd_shift=0, use="test")
    dl_test = DataLoader(
        ds_test,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        drop_last=False,
        pin_memory=pin_memory,
    )

    return dl_train, dl_valid, dl_test

================
File: data/DeepsetsDataset.py
================
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset

# Download the normalized_data3 folder with the data from: https://drive.google.com/drive/folders/1OrVEYa1JV8k6wtnq4qem1ZsMWLrAsYtx?usp=drive_link


class DeepSetDataset(Dataset):
    def __init__(self, data_files, target_files, use="train", train_frac=0.8, test_frac=0.1):
        self.data = []
        self.targets = []

        for data_file, target_file in zip(data_files, target_files):
            data = np.load(data_file)
            targets = np.load(target_file)
            self.data.append(data)
            self.targets.append(targets)

        total_samples = len(data_files)
        train_end = int(train_frac * total_samples)
        test_start = int((1 - test_frac) * total_samples)

        if use == "train":
            sti, edi = 0, train_end
        elif use == "validation":
            sti, edi = train_end, test_start
        elif use == "test":
            sti, edi = test_start, None
        else:
            raise ValueError(
                f"Unsupported use: {use}. This class should be used for building training, validation, or test set"
            )

        for data_file, target_file in zip(data_files[sti:edi], target_files[sti:edi]):
            data = np.load(data_file)
            targets = np.load(target_file)
            self.data.append(data)
            self.targets.append(targets)
        print(f"Loaded {len(self.data)} files for {use} set")

    def __getitem__(self, index):
        fold_idx = index // len(self.data[0])
        item_idx = index % len(self.data[0])
        data = self.data[fold_idx][item_idx]
        target = self.targets[fold_idx][item_idx]
        return torch.from_numpy(data).permute((1, 0)), torch.from_numpy(target)

    def __len__(self):
        return len(self.data) * len(self.data[0])


def setup_data_loaders_deepsets(base_file_name, batch_size=32, num_workers=4, pin_memory=False, prefetch_factor=2):
    train_data_files = [f"./data/normalized_data3/x_train_{base_file_name}.npy"]
    train_target_files = [f"./data/normalized_data3/y_train_{base_file_name}.npy"]
    test_data_files = [f"./data/normalized_data3/x_test_{base_file_name}.npy"]
    test_target_files = [f"./data/normalized_data3/y_test_{base_file_name}.npy"]

    ds_train = DeepSetDataset(train_data_files, train_target_files, use="train")
    dl_train = DataLoader(
        ds_train,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        drop_last=True,
        pin_memory=pin_memory,
    )

    ds_valid = DeepSetDataset(train_data_files, train_target_files, use="validation")
    dl_valid = DataLoader(
        ds_valid,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        drop_last=False,
        pin_memory=pin_memory,
    )

    ds_test = DeepSetDataset(test_data_files, test_target_files, use="test")
    dl_test = DataLoader(
        ds_test,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        drop_last=False,
        pin_memory=pin_memory,
    )

    return dl_train, dl_valid, dl_test

================
File: data/get_dataset.py
================
import os
import tarfile

tar_gz_path = "./data/dataset.tar.gz"
target_directory = "./data/"


# Function to extract the tar.gz file
def extract_tar_gz(tar_gz_path, target_directory):
    # Open the tar.gz file
    with tarfile.open(tar_gz_path, "r:gz") as tar:
        # Extract its contents into the target directory
        tar.extractall(path=target_directory)
    print(f"Files extracted to {target_directory}")


extract_tar_gz(tar_gz_path, target_directory)

================
File: examples/BraggNN/bragg_model_example_configs.yaml
================
example1:
  Proj_outchannel: 3
  b0: "None"
  b1: "Conv"
  b2: "Conv"
  b1_Conv_channels_0: 4
  b1_Conv_channels_1: 5
  b1_Conv_kernels_0: 1
  b1_Conv_kernels_1: 3
  b1_Conv_norms_0: "batch"
  b1_Conv_norms_1: null
  b1_Conv_acts_0: "ReLU"     
  b1_Conv_acts_1: "LeakyReLU" 
  b2_Conv_channels_0: 1
  b2_Conv_channels_1: 1
  b2_Conv_kernels_0: 3
  b2_Conv_kernels_1: 3
  b2_Conv_norms_0: null
  b2_Conv_norms_1: null
  b2_Conv_acts_0: "LeakyReLU"  
  b2_Conv_acts_1: "LeakyReLU" 
  MLP_width_0: 1
  MLP_width_1: 2
  MLP_width_2: 4
  MLP_acts_0: "Identity"      
  MLP_acts_1: "Identity"      
  MLP_acts_2: "LeakyReLU"      
  MLP_acts_3: "ReLU"         
  MLP_norms_0: "batch"
  MLP_norms_1: "batch"
  MLP_norms_2: "batch"
  MLP_norms_3: null

example2:
  Proj_outchannel: 0
  b0: "Conv"
  b1: "Conv"
  b2: "Conv"
  b0_Conv_channels_0: 5
  b0_Conv_channels_1: 4
  b0_Conv_kernels_0: 3
  b0_Conv_kernels_1: 3
  b0_Conv_norms_0: "batch"
  b0_Conv_norms_1: "batch"
  b0_Conv_acts_0: "ReLU"      
  b0_Conv_acts_1: "ReLU"      
  b1_Conv_channels_0: 2
  b1_Conv_channels_1: 2
  b1_Conv_kernels_0: 3
  b1_Conv_kernels_1: 3
  b1_Conv_norms_0: "batch"
  b1_Conv_norms_1: "batch"
  b1_Conv_acts_0: "LeakyReLU" 
  b1_Conv_acts_1: "LeakyReLU"  
  b2_Conv_channels_0: 2
  b2_Conv_channels_1: 2
  b2_Conv_kernels_0: 1
  b2_Conv_kernels_1: 3
  b2_Conv_norms_0: "batch"
  b2_Conv_norms_1: "batch"
  b2_Conv_acts_0: "GELU"      
  b2_Conv_acts_1: "ReLU"       
  MLP_width_0: 1
  MLP_width_1: 4
  MLP_width_2: 3
  MLP_acts_0: "ReLU"         
  MLP_acts_1: "GELU"         
  MLP_acts_2: "ReLU"         
  MLP_acts_3: "ReLU"         
  MLP_norms_0: null
  MLP_norms_1: "batch"
  MLP_norms_2: "batch"
  MLP_norms_3: null

## Example 3
example3:
  Proj_outchannel: 2
  b0: "Conv"
  b1: "None"
  b2: "Conv"
  b0_Conv_channels_0: 2
  b0_Conv_channels_1: 2
  b0_Conv_kernels_0: 1
  b0_Conv_kernels_1: 1
  b0_Conv_norms_0: null
  b0_Conv_norms_1: null
  b0_Conv_acts_0: "Identity"   
  b0_Conv_acts_1: "Identity"  
  b2_Conv_channels_0: 1
  b2_Conv_channels_1: 4
  b2_Conv_kernels_0: 1
  b2_Conv_kernels_1: 1
  b2_Conv_norms_0: "batch"
  b2_Conv_norms_1: "batch"
  b2_Conv_acts_0: "ReLU"     
  b2_Conv_acts_1: "ReLU"      
  MLP_width_0: 1
  MLP_width_1: 2
  MLP_width_2: 3
  MLP_acts_0: "GELU"        
  MLP_acts_1: "ReLU"         
  MLP_acts_2: "GELU"        
  MLP_acts_3: "ReLU"        
  MLP_norms_0: "batch"
  MLP_norms_1: "batch"
  MLP_norms_2: "batch"
  MLP_norms_3: null

## OpenHLS
openhls:
  b0: "ConvAttn"
  b1: "Conv"
  b2: "None"
  Proj_outchannel: 1
  b0_ConvAttn_hiddenchannel: 3
  b0_ConvAttn_act: "ReLU"    
  b1_Conv_channels_0: 2
  b1_Conv_channels_1: 0
  b1_Conv_kernels_0: 3
  b1_Conv_kernels_1: 3
  b1_Conv_acts_0: "ReLU"     
  b1_Conv_acts_1: "ReLU"     
  b1_Conv_norms_0: null
  b1_Conv_norms_1: null
  MLP_width_0: 2
  MLP_width_1: 1
  MLP_width_2: 0
  MLP_acts_0: "ReLU"        
  MLP_acts_1: "ReLU"        
  MLP_acts_2: "ReLU"         
  MLP_acts_3: "ReLU"       
  MLP_norms_0: null
  MLP_norms_1: null
  MLP_norms_2: null
  MLP_norms_3: null

## BraggNN
braggnn:
  b0: "ConvAttn"
  b1: "Conv"
  b2: "None"
  Proj_outchannel: 3
  b0_ConvAttn_hiddenchannel: 5
  b0_ConvAttn_act: "GELU"    
  b1_Conv_channels_0: 4
  b1_Conv_channels_1: 2
  b1_Conv_kernels_0: 3
  b1_Conv_kernels_1: 3
  b1_Conv_acts_0: "GELU"     
  b1_Conv_acts_1: "GELU"    
  b1_Conv_norms_0: null
  b1_Conv_norms_1: null
  MLP_width_0: 4
  MLP_width_1: 3
  MLP_width_2: 2
  MLP_acts_0: "GELU"        
  MLP_acts_1: "GELU"         
  MLP_acts_2: "GELU"        
  MLP_acts_3: "Identity"    
  MLP_norms_0: null
  MLP_norms_1: null
  MLP_norms_2: null
  MLP_norms_3: null

================
File: examples/BraggNN/braggnn_search_space.yaml
================
search_spaces:
  channel_space: [4, 8, 16, 32, 64]
  mlp_width_space: [4, 8, 16, 32, 64]  #for MLP layers
  kernel_space: [1, 3, 5]
  act_space:
    - "ReLU"
    - "LeakyReLU"
    - "GELU"
    - "Identity"
  norm_space: [null, "batch", "layer"]
  block_types: ["Conv", "ConvAttn", "None"]
  conv_attn:
    hidden_channel_space: [1, 2, 4, 8, 16, 32]

hyperparameters:
  num_blocks: 3
  initial_img_size: 9
  output_dim: 2

================
File: examples/BraggNN/Global_search_Bragg.py
================
import sys
import os
import torch
import optuna
import yaml

# Add the root directory to the Python path
root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.append(root_dir)

from data.BraggnnDataset import setup_data_loaders

def run_braggnn_search():
    # Set device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    
    # Set hyperparameters
    batch_size = 4096
    num_workers = 8
    
    # Load configurations
    config_dir = os.path.join(root_dir, "examples")
    with open(os.path.join(config_dir, "BraggNN/braggnn_model_example_configs.yaml"), "r") as f:
        braggnn_configs = yaml.safe_load(f)
    
    # Setup data loaders
    base_file_name = "jet_images_c8_minpt2_ptetaphi_robust_fast"
    train_loader, val_loader, test_loader = setup_data_loaders(
        base_file_name,
        batch_size=batch_size,
        num_workers=num_workers,
        prefetch_factor=2,
        pin_memory=True
    )
    print("Loaded Dataset...")
    
    # Create and configure the study
    study = optuna.create_study(
        sampler=optuna.samplers.NSGAIISampler(population_size=20),
        directions=['minimize', 'minimize']
    )
    
    # Queue example architectures from config
    study.enqueue_trial(braggnn_configs['openhls'])
    study.enqueue_trial(braggnn_configs['braggnn'])
    study.enqueue_trial(braggnn_configs['example1'])
    study.enqueue_trial(braggnn_configs['example2'])
    study.enqueue_trial(braggnn_configs['example3'])
    
    # Import the objective function from the main script
    from global_search import BraggNN_objective
    
    # Run optimization
    study.optimize(BraggNN_objective, n_trials=1000)
    
    return study

if __name__ == "__main__":
    study = run_braggnn_search()
    
    # Print out the best trials
    print("\nBest trials:")
    trials = study.best_trials
    
    for trial in trials:
        print(f"\nTrial {trial.number}")
        print(f"Mean Distance: {trial.values[0]}")
        print(f"BOPs: {trial.values[1]}")
        print("Params:", trial.params)

================
File: examples/DeepSets/deepsets_model_example_configs.yaml
================
## Base
base:
  bottleneck_dim: 5
  aggregator_type: "mean"   
  phi_len: 3
  phi_MLP_width_0: 3
  phi_MLP_width_1: 3
  phi_MLP_acts_0: "ReLU"     
  phi_MLP_acts_1: "ReLU"    
  phi_MLP_acts_2: "ReLU"    
  phi_MLP_norms_0: null
  phi_MLP_norms_1: null
  phi_MLP_norms_2: null
  rho_len: 2
  rho_MLP_width_0: 2
  rho_MLP_acts_0: "ReLU"    
  rho_MLP_acts_1: "GELU"    
  rho_MLP_norms_0: null
  rho_MLP_norms_1: null

## Large
large:
  bottleneck_dim: 5
  aggregator_type: "mean"    
  phi_len: 2
  phi_MLP_width_0: 3
  phi_MLP_acts_0: "ReLU"    
  phi_MLP_acts_1: "ReLU"     
  phi_MLP_norms_0: "batch"
  phi_MLP_norms_1: "batch"
  rho_len: 3
  rho_MLP_width_0: 3
  rho_MLP_width_1: 4
  rho_MLP_acts_0: "ReLU"     
  rho_MLP_acts_1: "ReLU"    
  rho_MLP_acts_2: "LeakyReLU"
  rho_MLP_norms_0: "batch"
  rho_MLP_norms_1: null
  rho_MLP_norms_2: "batch"

## Medium
medium:
  bottleneck_dim: 4
  aggregator_type: "mean"    
  phi_len: 2
  phi_MLP_width_0: 3
  phi_MLP_acts_0: "ReLU"    
  phi_MLP_acts_1: "ReLU"     
  phi_MLP_norms_0: "batch"
  phi_MLP_norms_1: "batch"
  rho_len: 4
  rho_MLP_width_0: 4
  rho_MLP_width_1: 1
  rho_MLP_width_2: 3
  rho_MLP_acts_0: "ReLU"     
  rho_MLP_acts_1: "LeakyReLU" 
  rho_MLP_acts_2: "ReLU"     
  rho_MLP_acts_3: "ReLU"    
  rho_MLP_norms_0: "batch"
  rho_MLP_norms_1: "batch"
  rho_MLP_norms_2: "batch"
  rho_MLP_norms_3: "batch"

## Small
small:
  bottleneck_dim: 3
  aggregator_type: "mean"     
  phi_len: 2
  phi_MLP_width_0: 1
  phi_MLP_acts_0: "LeakyReLU" 
  phi_MLP_acts_1: "ReLU"    
  phi_MLP_norms_0: "batch"
  phi_MLP_norms_1: null
  rho_len: 3
  rho_MLP_width_0: 2
  rho_MLP_width_1: 2
  rho_MLP_acts_0: "LeakyReLU" 
  rho_MLP_acts_1: "ReLU"    
  rho_MLP_acts_2: "LeakyReLU" 
  rho_MLP_norms_0: "batch"
  rho_MLP_norms_1: "batch"
  rho_MLP_norms_2: null

## Tiny
tiny:
  bottleneck_dim: 4
  aggregator_type: "mean"    
  phi_len: 1
  phi_MLP_acts_0: "ReLU"     
  phi_MLP_norms_0: "batch"
  rho_len: 4
  rho_MLP_width_0: 1
  rho_MLP_width_1: 1
  rho_MLP_width_2: 0
  rho_MLP_acts_0: "ReLU"     
  rho_MLP_acts_1: "GELU"     
  rho_MLP_acts_2: "ReLU"    
  rho_MLP_acts_3: "ReLU"     
  rho_MLP_norms_0: "batch"
  rho_MLP_norms_1: null
  rho_MLP_norms_2: null
  rho_MLP_norms_3: "batch"

================
File: examples/DeepSets/deepsets_search_space.yaml
================
search_spaces:
  mlp_width_space: [8, 16, 32, 64, 128]
  act_space:
    - "ReLU"
    - "LeakyReLU"
    - "GELU"
    - "Identity"
  norm_space: [null, "batch", "layer"]
  aggregator_space: ["mean", "max"]
  bottleneck_range: [0, 6]  # for 2^x, resulting in range [1, 64]

hyperparameters:
  phi_len_range: [1, 4]
  rho_len_range: [1, 4]

================
File: examples/DeepSets/Global_search_DeepSets.py
================
import sys
import os
import torch
import optuna
import yaml

# Add the root directory to the Python path
root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.append(root_dir)

from data.DeepsetsDataset import setup_data_loaders

def run_deepsets_search():
    # Set device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    
    # Set hyperparameters
    batch_size = 4096
    num_workers = 8
    
    # Load configurations
    config_dir = os.path.join(root_dir, "examples")
    with open(os.path.join(config_dir, "DeepSets/deepsets_model_example_configs.yaml"), "r") as f:
        deepsets_configs = yaml.safe_load(f)
    
    # Setup data loaders
    base_file_name = "jet_images_c8_minpt2_ptetaphi_robust_fast"
    train_loader, val_loader, test_loader = setup_data_loaders(
        base_file_name,
        batch_size=batch_size,
        num_workers=num_workers,
        prefetch_factor=2,
        pin_memory=True
    )
    print("Loaded Dataset...")
    
    # Create and configure the study
    study = optuna.create_study(
        sampler=optuna.samplers.NSGAIISampler(population_size=20),
        directions=['maximize', 'minimize']  # Note: DeepSets maximizes accuracy while minimizing BOPs
    )
    
    # Queue example architectures from config
    study.enqueue_trial(deepsets_configs['base'])
    study.enqueue_trial(deepsets_configs['large'])
    study.enqueue_trial(deepsets_configs['medium'])
    study.enqueue_trial(deepsets_configs['small'])
    study.enqueue_trial(deepsets_configs['tiny'])
    
    # Import the objective function from the main script
    from global_search import Deepsets_objective
    
    # Run optimization
    study.optimize(Deepsets_objective, n_trials=1000)
    
    return study

if __name__ == "__main__":
    study = run_deepsets_search()
    
    # Print out the best trials
    print("\nBest trials:")
    trials = study.best_trials
    
    for trial in trials:
        print(f"\nTrial {trial.number}")
        print(f"Accuracy: {trial.values[0]}")  # First objective is accuracy for DeepSets
        print(f"BOPs: {trial.values[1]}")
        print("Params:", trial.params)

================
File: models/blocks.py
================
import torch
import torch.nn as nn
import brevitas.nn as qnn

#NOTE: BraggNN does not divide by sqrt(d) like in traditional trasnformers
class ConvAttn(torch.nn.Module):
    def __init__(self, in_channels = 16, hidden_channels = 8, norm = None, act = None):
        super().__init__()
        self.hidden_channels = hidden_channels
        self.Wq = nn.Conv2d(in_channels, hidden_channels, kernel_size=1, stride=1)
        self.Wk = nn.Conv2d(in_channels, hidden_channels, kernel_size=1, stride=1)
        self.Wv = nn.Conv2d(in_channels, hidden_channels, kernel_size=1, stride=1)
        self.softmax = nn.Softmax(dim=-1)
        self.proj = nn.Conv2d(hidden_channels, in_channels, kernel_size=1, stride=1)
        self.act = act

    def forward(self, x):
        b, c, h, w = x.size()
        #q shape (b, seq, embed_dim) -> permute -> (b, embed_dim, seq)
        query = self.Wq(x).view(b, self.hidden_channels, -1).permute(0, 2, 1)
        key = self.Wk(x).view(b, self.hidden_channels, -1)
        value = self.Wv(x).view(b, self.hidden_channels, -1).permute(0, 2, 1)

        z = self.softmax(torch.matmul(query,key)) 
        z = torch.matmul(z, value).permute(0, 2, 1).view(b, self.hidden_channels, h, w)
        
        x = x + self.proj(z)
        if self.act is not None:
            x = self.act(x)
        return x

class ConvBlock(torch.nn.Module):
    def __init__(self, channels, kernels, acts, norms, img_size):
        super().__init__()
        self.layers = []
        for i in range(len(kernels)):
            self.layers.append( nn.Conv2d(channels[i], channels[i+1], 
                                          kernel_size=kernels[i], stride=1, 
                                          padding = 0 )) #padding = (kernels[i] - 1) // 2)
            if kernels[i] == 3: img_size -= 2
            if norms[i] == 'batch':
                self.layers.append( nn.BatchNorm2d(channels[i+1]) )
            elif norms[i] == 'layer':
                self.layers.append( nn.LayerNorm([channels[i+1], img_size, img_size]) ) #DEPRECATED
            if acts[i] != None:
                self.layers.append(acts[i])
        self.layers = nn.Sequential(*self.layers)
      
    def forward(self, x):
        return self.layers(x)

#TODO: Add variable length with pass through layers ie lambda x: x
class MLP(torch.nn.Module):
    def __init__(self, widths, acts, norms):
        super().__init__()

        self.layers = []
        for i in range(len(acts)): 
            self.layers.append( nn.Linear(widths[i], widths[i+1]) )
            if norms[i] == 'batch':
                self.layers.append( nn.BatchNorm1d(widths[i+1]) )
            elif norms[i] == 'layer':
                self.layers.append( nn.LayerNorm(widths[i+1]) ) #DEPRECATED
            #elif None, skip
            if acts[i] != None:
                self.layers.append( acts[i] )
        self.layers = nn.Sequential(*self.layers)
        

    def forward(self, x):
        return self.layers(x)

def get_activation(act_name: str) -> nn.Module:
    """Convert activation function name to PyTorch module"""
    act_map = {
        "ReLU": nn.ReLU(),
        "LeakyReLU": nn.LeakyReLU(negative_slope=0.01),
        "GELU": nn.GELU(),
        "Identity": lambda x: x
    }
    return act_map[act_name]

def sample_MLP(trial, in_dim, out_dim, prefix, search_space, num_layers=3):
    """Generic MLP sampling function using provided search space"""
    mlp_width_space = search_space["mlp_width_space"]
    act_space = search_space["act_space"]
    norm_space = search_space["norm_space"]

    # Create widths list
    widths = [in_dim]
    for i in range(num_layers-1):
        widths.append(mlp_width_space[trial.suggest_int(f"{prefix}_width_{i}", 0, len(mlp_width_space)-1)])
    widths.append(out_dim)

    # Sample activations
    acts = []
    for i in range(num_layers):
        act_name = trial.suggest_categorical(f"{prefix}_acts_{i}", act_space)
        acts.append(get_activation(act_name))

    # Sample normalizations
    norms = [trial.suggest_categorical(f"{prefix}_norms_{i}", norm_space) 
             for i in range(num_layers)]

    return widths, acts, norms

def sample_ConvBlock(trial, prefix, in_channels, search_space, num_layers=2):
    """Generic ConvBlock sampling using provided search space"""
    channel_space = search_space["channel_space"]
    kernel_space = search_space["kernel_space"]
    act_space = search_space["act_space"]
    norm_space = search_space["norm_space"]

    # Sample channels
    channels = [int(in_channels)]
    for i in range(num_layers):
        next_channel = channel_space[trial.suggest_int(f"{prefix}_channels_{i}", 
                                                     0, len(channel_space) - 1)]
        channels.append(next_channel)

    # Sample other parameters
    kernels = [trial.suggest_categorical(f"{prefix}_kernels_{i}", kernel_space) 
              for i in range(num_layers)]
    
    acts = []
    for i in range(num_layers):
        act_name = trial.suggest_categorical(f"{prefix}_acts_{i}", act_space)
        acts.append(get_activation(act_name))

    norms = [trial.suggest_categorical(f"{prefix}_norms_{i}", norm_space) 
             for i in range(num_layers)]

    return channels, kernels, acts, norms

def sample_ConvAttn(trial, prefix, search_space):
    """Generic ConvAttn sampling using provided search space"""
    hidden_channel_space = search_space["conv_attn"]["hidden_channel_space"]
    act_space = search_space["act_space"]
    
    hidden_channels = hidden_channel_space[
        trial.suggest_int(f"{prefix}_hiddenchannel", 0, len(hidden_channel_space) - 1)
    ]
    
    act_name = trial.suggest_categorical(f"{prefix}_act", act_space)
    act = get_activation(act_name)
    
    return hidden_channels, act




class CandidateArchitecture(torch.nn.Module):
    def __init__(self, Blocks, MLP, hidden_channels, input_channels = 1):
        super().__init__()
        self.conv = nn.Conv2d(input_channels, hidden_channels, kernel_size=(3, 3), stride=(1, 1)) #Initial Projection Layer
        self.Blocks = Blocks
        self.MLP = MLP

    def forward(self, x):
        x = self.conv(x)
        x = self.Blocks(x)
        x = torch.flatten(x, 1)
        x = self.MLP(x)
        return x
    
class DeepSetsArchitecture(torch.nn.Module):
    def __init__(self, phi, rho, aggregator):
        super().__init__()
        self.phi = phi
        self.rho = rho
        self.aggregator = aggregator

    def forward(self, x):
        x = self.phi(x)
        x = self.aggregator(x) #torch.mean(x, dim=1)
        #x = torch.squeeze(x, dim=1)
        x = self.rho(x)
        return x
    
class SequenceNorm1D(torch.nn.Module):
    def __init__(self, seq_len): 
        super().__init__()
        self.norm = nn.BatchNorm1d(seq_len)

    def forward(self, x):
        if len(x.size()) == 3:
            indices = (0,2,1)
        elif len(x.size()) == 2:
            indices = (0,1) #dont permute.
        else:
            print(f'Input x of size {x.size()} is not supported!')

        x = torch.permute(x, indices)
        x = self.norm(x)
        x = torch.permute(x, indices)
        return x

class Phi(torch.nn.Module):
    def __init__(self, widths, acts, norms):
        super().__init__()

        self.layers = []
        for i in range(len(acts)): 
            self.layers.append( nn.Linear(widths[i], widths[i+1]) )
            if norms[i] == 'batch':
                self.layers.append( nn.BatchNorm1d(8) )
            elif norms[i] == 'sequence':
                self.layers.append( SequenceNorm1D(widths[i+1]) )
            if acts[i] != None:
                self.layers.append( acts[i] )
        self.layers = nn.Sequential(*self.layers)
        

    def forward(self, x):
        return self.layers(x)
    
class Rho(torch.nn.Module):
    def __init__(self, widths, acts, norms):
        super().__init__()

        self.layers = []
        for i in range(len(acts)): 
            self.layers.append( nn.Linear(widths[i], widths[i+1]) )
            if norms[i] == 'batch':
                self.layers.append( nn.BatchNorm1d(widths[i+1]) )
            elif norms[i] == 'sequence':
                self.layers.append( SequenceNorm1D(widths[i+1]) )

            if acts[i] != None:
                self.layers.append( acts[i] )
        self.layers = nn.Sequential(*self.layers)
        

    def forward(self, x):
        return self.layers(x)
    
class ConvPhi(torch.nn.Module):
    def __init__(self, widths, acts, norms):
        super().__init__()

        self.layers = []
        for i in range(len(acts)): 
            self.layers.append( nn.Conv1d(widths[i], widths[i+1], kernel_size=1,stride=1) )
            if norms[i] == 'batch':
                self.layers.append( nn.BatchNorm1d(widths[i+1]) )
            elif norms[i] == 'sequence':
                self.layers.append( SequenceNorm1D(widths[i+1]) )
            if acts[i] != None:
                self.layers.append( acts[i] )
        self.layers = nn.Sequential(*self.layers)
        

    def forward(self, x):
        return self.layers(x)
    
class QAT_Phi(torch.nn.Module):
    def __init__(self, widths, acts, norms, bit_width=8):
        super().__init__()
        self.layers = nn.Sequential()
        self.quant_inp = qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)
        for i in range(len(acts)):
            linear_layer = qnn.QuantLinear(widths[i], widths[i+1], bias=True, weight_bit_width=bit_width)
            self.layers.add_module(f'linear_{i}', linear_layer)
            if norms[i] == 'batch':
                self.layers.add_module(f'norm_{i}', nn.BatchNorm1d(8))
            if acts[i] is not None:
                act_layer = acts[i] if isinstance(acts[i], nn.Module) else qnn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)
                self.layers.add_module(f'act_{i}', act_layer)
                
    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = self.quant_inp(x)
            x = layer(x)
        return x
    
class QAT_ConvPhi(torch.nn.Module):
    def __init__(self, widths, acts, norms, bit_width=8):
        super().__init__()
        self.layers = nn.Sequential()
        self.quant_inp = qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)
        for i in range(len(acts)):
            linear_layer = qnn.QuantConv1d(widths[i], widths[i+1], kernel_size=1, stride=1, weight_bit_width=bit_width)
            self.layers.add_module(f'linear_{i}', linear_layer)
            if norms[i] == 'batch':
                self.layers.add_module(f'norm_{i}', nn.BatchNorm1d(widths[i+1]))
            if acts[i] is not None:
                act_layer = acts[i] if isinstance(acts[i], nn.Module) else qnn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)
                self.layers.add_module(f'act_{i}', act_layer)
                
    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = self.quant_inp(x)
            x = layer(x)
        return x
    
class QAT_Rho(torch.nn.Module):
    def __init__(self, widths, acts, norms, bit_width=8):
        super().__init__()
        self.layers = nn.Sequential()
        self.quant_inp = qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)
        for i in range(len(acts)):
            linear_layer = qnn.QuantLinear(widths[i], widths[i+1], bias=True, weight_bit_width=bit_width)
            self.layers.add_module(f'linear_{i}', linear_layer)
            if norms[i] == 'batch':
                self.layers.add_module(f'norm_{i}', nn.BatchNorm1d(widths[i+1]))
            elif norms[i] == 'layer':
                self.layers.add_module(f'norm_{i}', nn.LayerNorm(widths[i+1]))
            if acts[i] is not None:
                act_layer = acts[i] if isinstance(acts[i], nn.Module) else qnn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)
                self.layers.add_module(f'act_{i}', act_layer)
                
    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = self.quant_inp(x)
            x = layer(x)
        return x

class Identity(torch.nn.Module):
    def __init__(self):
        super(self).__init__()
    
    def forward(self, x):
        return x

class QAT_ConvAttn(torch.nn.Module):
    def __init__(self, in_channels = 16, hidden_channels = 8, norm = None, act = None, bit_width=8):
        super().__init__()
        self.quant_inp = qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)
        self.hidden_channels = hidden_channels
        self.Wq = qnn.QuantConv2d(in_channels, hidden_channels, kernel_size=1, stride=1, weight_bit_width=bit_width)
        self.Wk = qnn.QuantConv2d(in_channels, hidden_channels, kernel_size=1, stride=1, weight_bit_width=bit_width)
        self.Wv = qnn.QuantConv2d(in_channels, hidden_channels, kernel_size=1, stride=1, weight_bit_width=bit_width)
        self.softmax = nn.Softmax(dim=-1) # kept in floating point
        self.proj = qnn.QuantConv2d(hidden_channels, in_channels, kernel_size=1, stride=1, weight_bit_width=bit_width)
        self.act = act if act is not None else qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)
        # Initialize the QuantIdentity layer for softmax output
        #self.quant_identity = qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)
    def forward(self, x):
        x = self.quant_inp(x)
        #print("Entering ConvAttn - Input shape:", x.shape)
        b, c, h, w = x.size()
        query = self.Wq(x).view(b, self.hidden_channels, -1).permute(0, 2, 1)
        key = self.Wk(x).view(b, self.hidden_channels, -1)
        value = self.Wv(x).view(b, self.hidden_channels, -1).permute(0, 2, 1)
        z = self.softmax(torch.matmul(query,key))
        z = torch.matmul(z, value).permute(0, 2, 1).view(b, self.hidden_channels, h, w)
        z = self.quant_inp(z) #z = self.quant_identity(z)
        x = x + self.proj(z)
        if self.act is not None:
            x = self.act(x)
        return x
   
class QAT_ConvBlock(nn.Module):
    def __init__(self, channels, kernels, acts, norms, img_size, bit_width=8):
        super().__init__()
        self.layers = nn.Sequential()
        self.quant_inp = qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)
        for i in range(len(kernels)):
            conv = qnn.QuantConv2d(channels[i], channels[i+1], kernel_size=kernels[i], stride=1, padding=0, weight_bit_width=bit_width)
            self.layers.append(conv)
            if kernels[i] == 3: img_size -= 2
            if norms[i] == 'batch':
                norm_layer = nn.BatchNorm2d(channels[i+1])
                self.layers.append(norm_layer)
            elif norms[i] == 'layer': #DEPRECATED
                norm_layer = nn.LayerNorm([channels[i+1], img_size, img_size])
                self.layers.append(norm_layer)
            if norms[i] is not None:
                self.layers.append(qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True))
            if acts[i] is not None:
                act_layer = acts[i] if isinstance(acts[i], nn.Module) else qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)
                self.layers.append(act_layer)

    def forward(self, x):
        #print("entering block")
        for layer in self.layers:
            x = self.quant_inp(x)
            x = layer(x)
        #print("exiting block")
        return x
    
class QAT_MLP(torch.nn.Module):
    def __init__(self, widths, acts, norms, bit_width=8):
        super().__init__()
        self.layers = nn.Sequential()
        self.quant_inp = qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)
        for i in range(len(acts)):
            linear_layer = qnn.QuantLinear(widths[i], widths[i+1], bias=True, weight_bit_width=bit_width)
            self.layers.add_module(f'linear_{i}', linear_layer)
            if norms[i] == 'batch':
                self.layers.add_module(f'norm_{i}', nn.BatchNorm1d(widths[i+1]))
            elif norms[i] == 'layer':
                self.layers.add_module(f'norm_{i}', nn.LayerNorm(widths[i+1]))
            if acts[i] is not None:
                act_layer = acts[i] if isinstance(acts[i], nn.Module) else qnn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)
                self.layers.add_module(f'act_{i}', act_layer)
                
    def forward(self, x):
        x = self.quant_inp(x)
        for i, layer in enumerate(self.layers):
            x = self.quant_inp(x)
            x = layer(x)
        return x
    
class QAT_CandidateArchitecture(torch.nn.Module):
    def __init__(self, Blocks, MLP, hidden_channels, input_channels=1, bit_width=8):
        super().__init__()
        self.quant_inp = qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True) # initial layer to initialize quantization
        self.conv = qnn.QuantConv2d(input_channels, hidden_channels, kernel_size=(3, 3), # initial projection layer quantized
                                    stride=(1, 1), weight_bit_width=bit_width)
        self.Blocks = Blocks
        self.MLP = MLP
    def forward(self, x):
        x = self.quant_inp(x)
        x = self.conv(x)
        x = self.Blocks(x)
        x = torch.flatten(x, 1)
        x = self.MLP(x)
        return x
    

    

#Leaving this in for later, currently not used/working.
class TransformerBlock(torch.nn.Module):
    def __init__(self, input_size = (4, 16, 9, 9)):
        super().__init__()
        embed_dim = input_size[-2] * input_size[-1]
               
        #Sample parameters for LinearAttention, rewrite with optuna samplers.
        trial = {
            'num_heads' : 1, #pick from [1,2,4,6,8]
            'norm' : nn.BatchNorm1d(input_size[1]), #pick from [nn.BatchNorm1d(input_size[1]), nn.LayerNorm((4,16,81))]
            'hidden_dim_scale' : 2, #pick from [1,2,4]
            'dropout' : .1, #float
            'bias' : True, #[True, False]
            'num_layers': 1, #[1,2,3]
        }

        self.layers = [ nn.TransformerEncoderLayer(d_model=embed_dim,
                                  nhead=trial['num_heads'],
                                  dim_feedforward=embed_dim * trial['hidden_dim_scale'],
                                  dropout=trial['dropout'],
                                  bias=trial['bias']) for i in range(trial['num_layers'])]

    def forward(self, x):
        x = torch.flatten(x, 2) #now (4, 16, 81)
        for l in self.layers:
          x = l(x)

#Leaving this in for later, currently not used/working.
class SkipBlock(torch.nn.Module):
    def __init__(self):
        super().__init__()
        #Sample hyperparameters
        self.input_size = (4,16,11,11)
        self.channels = [16,4,4,16]
        self.kernels = [1,3,1] #[1,3,5]
        self.act = [nn.ReLU(), nn.ReLU(), lambda x: x, nn.ReLU()] #pick from [nn.ReLU(), nn.LeakyReLU(), nn.GELU(), lambda x: x]
        self.norm = ['batch', 'batch', 'batch'] #pick from ['identity', 'layer', 'batch']

        self.layers = []
        for i in range(len(self.kernels)):
            self.layers.append( nn.Conv2d(self.channels[i], self.channels[i+1], 
                                          kernel_size=self.kernels[i], stride=1, 
                                          padding = (self.kernels[i] - 1) // 2) )
            if self.norm[i] == 'batch':
                self.layers.append( nn.BatchNorm2d(self.channels[i+1]) )
            elif self.norm[i] == 'layer':
                self.layers.append( nn.LayerNorm(self.input_size) )

            self.layers.append( self.act[i] )
      

    def forward(self, x):
        z = x 
        for l in self.layers:
            z = l(z)
        x += z
        x = self.act[-1](x) #Activation after skip
        return x

================
File: utils/bops.py
================
import math

import torch
import torch.nn as nn


# Returns sparsity calculated as percentage of zeros in a tensor
def get_sparsity(tensor):
    num_zeros = torch.sum(tensor == 0)
    total_params = tensor.numel()
    return num_zeros / total_params


"""
Calculate the bops in the matrix multiplication in attention
Let a,b be the shape of matrices A & B
For the QK matmul, we perform a dot product across the embedding dimension, for each row and column,
so there are seq_len^2 many dot products. Each dot product uses embed_dim many multiplications and
embed_dim - 1 additions.

DISCLAIMER: This only calculates bops for dense matmuls
"""


def get_matmul_bops(a, b, bit_width=32):
    if a[0] != b[0] or a[1] != b[2]:
        raise ValueError("Inner dimensions of arrays do not match for matrix multiplication.")
    batch_size = a[0]
    embed_dim = a[1]
    seq_len = a[2]

    bops_per_mult = bit_width**2
    bops_per_add = bit_width

    mult_bops = seq_len * seq_len * embed_dim * bops_per_mult
    add_bops = seq_len * seq_len * (embed_dim - 1) * bops_per_add
    bops = mult_bops + add_bops
    return bops


def get_linear_bops(layer, bit_width=32):
    sparsity = get_sparsity(layer.weight.data)
    return (
        layer.out_features
        * layer.in_features
        * ((1 - sparsity) * bit_width**2 + 2 * bit_width + math.log2(layer.in_features))
    )


def get_conv2d_bops(layer, input_shape, bit_width=32):
    output_spatial_dim = input_shape[-1] if layer.kernel_size == 1 else input_shape[-1] - 2
    output_shape = (input_shape[0], layer.out_channels, output_spatial_dim, output_spatial_dim)

    input_numel = torch.prod(torch.tensor(input_shape[1:]))
    output_numel = torch.prod(torch.tensor(output_shape[1:]))

    sparsity = get_sparsity(layer.weight.data)
    return (
        output_numel
        * input_numel
        * layer.kernel_size[0] ** 2
        * ((1 - sparsity) * bit_width**2 + 2 * bit_width + math.log2(input_numel * layer.kernel_size[0] ** 2))
    )


def get_Conv_bops(block, input_shape, bit_width=32):
    bops = 0
    for i, layer in enumerate(block.layers):
        if isinstance(layer, nn.Conv2d):
            sparsity = get_sparsity(layer.weight.data)
            bops += get_conv2d_bops(layer, input_shape, bit_width)

            # Update input_shape for future Conv2D layers
            output_spatial_dim = input_shape[-1] if layer.kernel_size == 1 else input_shape[-1] - 2
            input_shape = (input_shape[0], layer.out_channels, output_spatial_dim, output_spatial_dim)

    return bops


def get_ConvAttn_bops(block, input_shape=(64, 1, 9, 9), bit_width=32):
    bops = 0

    # Add bops for each Wk, Wq, Wv, Proj
    qkv_layers = [block.Wk, block.Wq, block.Wv]
    for layer in qkv_layers:
        bops += get_conv2d_bops(layer, input_shape, bit_width)

    hidden_shape = (input_shape[0], block.hidden_channels, input_shape[2], input_shape[3])
    bops += get_conv2d_bops(block.proj, input_shape, bit_width)

    # Get Input Shape and Reshaped dims
    batch_size, seq_len, h, w = input_shape
    embed_dim = h * w

    # Add softmax bops
    bops += (
        batch_size * embed_dim**2 * 1.5 * (bit_width - 1)
        + batch_size * embed_dim * (embed_dim - 1)
        + batch_size * (embed_dim) ** 2
    )

    # Add QK MatMul bops
    Q_shape = (batch_size, embed_dim, seq_len)
    K_shape = (batch_size, seq_len, embed_dim)
    bops += get_matmul_bops(Q_shape, K_shape, bit_width=32)

    # Add SV Matmul bops
    S_shape = (batch_size, seq_len, seq_len)  # S is the output scores from softmax
    V_shape = (batch_size, embed_dim, seq_len)
    bops += get_matmul_bops(S_shape, V_shape, bit_width=32)

    return bops


def get_MLP_bops(block, bit_width=32):
    bops = 0
    for i, layer in enumerate(block.layers):
        if isinstance(layer, nn.Linear):
            bops += get_linear_bops(layer, bit_width)
    return bops


def get_AvgPool_bops(input_shape, dim=1, bit_width=32):
    # number of elements in the dimension to be reduced
    num_elements_in_dim = input_shape[dim]

    # Calculate the number of elements in the output tensor
    output_elements = 1
    for i, d in enumerate(input_shape):
        if i != dim:
            output_elements *= d

    # bit operations for summing up the elements
    sum_bit_operations = (num_elements_in_dim - 1) * output_elements * bit_width  # similar to how we calculated sum

    div_bit_operations = (
        output_elements * math.log2(output_elements) * bit_width
    )  # Similar to how we previosuly calculated division

    # memory access operations for reading the input tensor
    input_elements = math.prod(input_shape)
    read_ops = input_elements * math.log2(input_elements)

    # memry access operations for writing the output tensor
    # write_ops = output_elements * math.log2(output_elements)

    total_bit_operations = sum_bit_operations + div_bit_operations + read_ops

    return total_bit_operations


def get_MaxPool_bops(input_shape, dim=1, bit_width=32):

    # number of elements in the dimension to be reduced
    num_elements_in_dim = input_shape[dim]

    # Calculate the number of elements in the output tensor
    output_elements = 1
    for i, d in enumerate(input_shape):
        if i != dim:
            output_elements *= d

    # max of an n-long tensor compares t[0] > t[1], max(t[0],t[1]) > t[2]... so n-1 comparisons. But we have num_elements_in_dim many tensors.
    num_comparisons = (output_elements - 1) * num_elements_in_dim

    # worst case time complexity is O(n) becuase you are iterating through all the bits to see which is larger.
    bops_per_comparison = bit_width

    input_elements = math.prod(input_shape)
    read_bops = input_elements * math.log2(input_elements)

    bops = num_comparisons * bops_per_comparison + read_bops
    return bops


# NOTE: BatchNorm, LayerNorm, etc. are extremely small relative to the Conv2D, linear, and matmul operations. We skip these as they are negligible.

================
File: utils/metrics.py
================
import os
import time
from datetime import datetime
from functools import partial

import numpy as np
import optuna
import torch
import torch.nn as nn
from torch.optim.lr_scheduler import _LRScheduler
from torch.utils.data import DataLoader
from tqdm import tqdm


# Metric Functions
def get_mean_dist(model, dataloader, device, psz=11):
    distances = []
    with torch.no_grad():
        for features, true_locs in dataloader:
            features = features.to(device)
            preds = model(features)  # assuming model outputs normalized [px, py]
            preds = preds.cpu().numpy()

            # Calculate Euclidean distance
            distance = np.sqrt(np.sum((preds - true_locs.numpy()) ** 2, axis=1)) * 11  # psz=11
            distances.extend(distance)  # Changed from append to extend

    mean_distance = np.mean(distances)
    return mean_distance


def get_param_count_BraggNN(model):
    count = 0
    count += sum(p.numel() for p in model.Blocks.parameters())
    count += sum(p.numel() for p in model.MLP.parameters())
    count += sum(p.numel() for p in model.conv.parameters())
    return count


def get_param_count_Deepsets(model):
    count = 0
    count += sum(p.numel() for p in model.phi.parameters())
    count += sum(p.numel() for p in model.rho.parameters())
    return count


def get_inference_time(model, device, img_size=(256, 1, 11, 11)):
    x = torch.randn(img_size).to(device)
    start = time.time()
    for _ in range(100):
        y = model(x)
    end = time.time()
    return end - start


def get_acc(model, dataloader, device):
    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        for data, targets in dataloader:
            data = data.to(device).float()
            targets = targets.to(device).float()

            outputs = model(data)
            _, predicted = torch.max(outputs, 1)
            true_labels = torch.argmax(targets, 1)  # Get the true class labels

            total += true_labels.size(0)
            correct += (predicted == true_labels).sum().item()

        accuracy = correct / total
        print(f"Test Accuracy: {accuracy:.4f}")

    return accuracy

================
File: utils/processor.py
================
import os
import time
from datetime import datetime
from functools import partial

import numpy as np
import optuna
import torch
import torch.nn as nn
from torch.optim.lr_scheduler import ReduceLROnPlateau, _LRScheduler
from torch.utils.data import DataLoader
from tqdm import tqdm

from .metrics import get_acc, get_inference_time, get_mean_dist, get_param_count_BraggNN, get_param_count_Deepsets


# Trains BraggNN models and calculates all metrics
def evaluate_BraggNN(model, train_loader, val_loader, device, num_epochs=50, lr=0.0015, weight_decay=2.2e-9):
    model = model.to(device)

    # Train Model
    criterion = torch.nn.MSELoss()
    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    validation_loss = train(model, optimizer, scheduler, criterion, train_loader, val_loader, device, num_epochs)

    # Evaluate Performance
    mean_distance = get_mean_dist(model, val_loader, device)
    # Evaluate Efficiency
    param_count = get_param_count_BraggNN(
        model
    )  # Just for reference, we are not optimizing for this. We measure BOPs in global_search.py
    inference_time = get_inference_time(
        model, device, img_size=(256, 1, 11, 11)
    )  # Just for reference, we are not optimizing for this.

    print(
        "Mean Distance: ",
        mean_distance,
        ", Inference time: ",
        inference_time,
        ", Validation Loss: ",
        validation_loss,
        ", Param Count: ",
        param_count,
    )
    return mean_distance, inference_time, validation_loss, param_count




def evaluate_deepsets(model, train_loader, val_loader, test_loader, device, num_epochs=100, lr=0.0032):
    """Evaluates DeepSets models by training and computing performance metrics"""
    # Move model to device
    model = model.to(device)
    
    # Initialize training components
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.1, patience=3)
    
    # Train model
    validation_loss = train(model, optimizer, scheduler, criterion, 
                          train_loader, val_loader, device, 
                          num_epochs, patience=7)
    
    # Calculate metrics
    val_accuracy = get_acc(model, val_loader, device)
    test_accuracy = get_acc(model, test_loader, device)
    param_count = get_param_count_Deepsets(model)
    inference_time = get_inference_time(model, device, img_size=(1024, 3, 8))
    
    # Print results
    print(
        f"Validation Accuracy: {val_accuracy:.4f}, "
        f"Test Accuracy: {test_accuracy:.4f}, "
        f"Inference time: {inference_time:.4f}, "
        f"Validation Loss: {validation_loss:.4f}, "
        f"Parameter Count: {param_count}"
    )
    
    # Return metrics dictionary
    return {
        "val_accuracy": val_accuracy,
        "test_accuracy": test_accuracy,
        "val_loss": validation_loss,
        "inference_time": inference_time,
        "param_count": param_count
    }

def train(model, optimizer, scheduler, criterion, train_loader, valid_loader, device, num_epochs, patience=5):
    curr_patience = patience
    previous_epoch_loss = float("inf")

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        for i, batch in enumerate(train_loader):
            optimizer.zero_grad()
            inputs, targets = batch
            inputs, targets = inputs.to(device).float(), targets.to(device).float()

            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

        # Validation phase
        model.eval()
        validation_loss = 0
        with torch.no_grad():
            for batch in valid_loader:
                inputs, targets = batch
                inputs, targets = inputs.to(device).float(), targets.to(device).float()

                outputs = model(inputs)
                loss = criterion(outputs, targets)
                validation_loss += loss.item()

        validation_loss /= len(valid_loader)

        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(validation_loss)
        else:
            scheduler.step()

        # Early Stopping Procedure
        if validation_loss < previous_epoch_loss:
            curr_patience = patience
        else:
            curr_patience -= 1
            if curr_patience <= 0:
                break
        previous_epoch_loss = validation_loss

    return previous_epoch_loss

================
File: .gitignore
================
# Ignore everything in the dataset directory
/data/frames-exp4train.hdf5
/data/peaks-exp4train-psz11.hdf5
env/
.env/
/saved_models/*
data/normalized_data3/
data/normalized_data3-20240423T200227Z-001.zip
data/normalized_data3-20240430T145906Z-001.zip
*.pyc
__pycache__/
Results/
plots/

================
File: .pre-commit-config.yaml
================
repos:
- repo: https://github.com/psf/black
  rev: 24.10.0
  hooks:
  - id: black
    language_version: python3
    args: ["--line-length=125"]

- repo: https://github.com/pre-commit/pre-commit-hooks
  rev: v5.0.0
  hooks:
  - id: check-added-large-files
  - id: check-case-conflict
  - id: check-merge-conflict
  - id: check-symlinks
  - id: check-yaml
  - id: debug-statements
  - id: end-of-file-fixer
  - id: mixed-line-ending
  - id: requirements-txt-fixer
  - id: trailing-whitespace

- repo: https://github.com/PyCQA/isort
  rev: 5.13.2
  hooks:
  - id: isort
    args: ["--profile", "black", --line-length=125]

- repo: https://github.com/asottile/pyupgrade
  rev: v3.19.0
  hooks:
  - id: pyupgrade
    args: ["--py36-plus"]

- repo: https://github.com/asottile/setup-cfg-fmt
  rev: v2.7.0
  hooks:
  - id: setup-cfg-fmt

- repo: https://github.com/pycqa/flake8
  rev: 7.1.1
  hooks:
  - id: flake8
    exclude: docs/conf.py
    additional_dependencies: [flake8-bugbear, flake8-print]
    args: ['--max-line-length=125',  # github viewer width
           '--extend-ignore=E203,T201']  # E203 is not PEP8 compliant

- repo: https://github.com/mgedmin/check-manifest
  rev: "0.50"
  hooks:
  - id: check-manifest
    stages: [manual]

================
File: global_search.py
================
# from data.BraggnnDataset import setup_data_loaders
import torch
import torch.nn as nn
import optuna

from models.blocks import *
from utils.bops import *
from utils.processor import evaluate_BraggNN, evaluate_deepsets
import yaml
import os

from data.BraggnnDataset import *
from data.DeepsetsDataset import *


"""
Optuna Objective to evaluate a trial
1) Samples architecture from hierarchical search space
2) Trains Model
3) Evaluates Mean Distance, bops, param count, inference time, and val loss
Saves all information in global_search.txt
"""


def load_configs(task="deepsets", config_dir="examples/"):
    """Load YAML configuration files based on specified task.
    
    Args:
        task (str): Task to load configs for. Either "deepsets" or "braggnn".
        config_dir (str): Directory containing config files.
        
    Returns:
        tuple: (task_configs, search_space) containing model configs and search space for specified task
        
    Raises:
        ValueError: If task is not "deepsets" or "braggnn"
    """
    if task not in ["deepsets", "braggnn"]:
        raise ValueError('Task must be either "deepsets" or "braggnn"')
        
    if task == "deepsets":
        with open(os.path.join(config_dir, "DeepSets/deepsets_search_space.yaml"), "r") as f:
            search_space = yaml.safe_load(f)
        
        with open(os.path.join(config_dir, "DeepSets/deepsets_model_example_configs.yaml"), "r") as f:
            task_configs = yaml.safe_load(f)
            
    else:  # task == "braggnn"
        with open(os.path.join(config_dir, "BraggNN/braggnn_search_space.yaml"), "r") as f:
            search_space = yaml.safe_load(f)
        
        with open(os.path.join(config_dir, "BraggNN/bragg_model_example_configs.yaml"), "r") as f:
            task_configs = yaml.safe_load(f)
    
    return task_configs, search_space


def BraggNN_objective(trial):
    """BraggNN objective using search space config"""
    task_configs, search_space = load_configs(task="braggnn")
    spaces = search_space["search_spaces"]
    hyper_params = search_space["hyperparameters"]
    
    num_blocks = hyper_params["num_blocks"]
    img_size = hyper_params["initial_img_size"]
    output_dim = hyper_params["output_dim"]
    
    # Sample first channel dimension
    block_channels = [spaces["channel_space"][
        trial.suggest_int("Proj_outchannel", 0, len(spaces["channel_space"]) - 1)
    ]]

    # Sample Block Types
    b = [trial.suggest_categorical(f"b{i}", spaces["block_types"]) 
         for i in range(num_blocks)]

    Blocks = []
    bops = 0

    # Build Blocks
    for i, block_type in enumerate(b):
        if block_type == "Conv":
            channels, kernels, acts, norms = sample_ConvBlock(
                trial, 
                f"b{i}_Conv", 
                block_channels[-1],
                search_space=spaces,  # Pass search space
                num_layers=2
            )
            
            reduce_img_size = 2 * sum([1 if k == 3 else 0 for k in kernels])
            while img_size - reduce_img_size <= 0:
                kernels[kernels.index(3)] = 1
                reduce_img_size = 2 * sum([1 if k == 3 else 0 for k in kernels])
            
            Blocks.append(ConvBlock(channels, kernels, acts, norms, img_size))

            bops += get_Conv_bops(Blocks[-1], input_shape=[batch_size, channels[0], img_size, img_size], bit_width=32)
            img_size -= reduce_img_size
            block_channels.append(channels[-1])

        elif block_type == "ConvAttn":
            hidden_channels, act = sample_ConvAttn(
                trial, 
                f"b{i}_ConvAttn",
                search_space=spaces  # Pass search space
            )
            Blocks.append(ConvAttn(block_channels[-1], hidden_channels, act))

            bops += get_ConvAttn_bops(
                Blocks[-1], 
                input_shape=[batch_size, block_channels[-1], img_size, img_size], 
                bit_width=32
            )

    # Build MLP
    in_dim = block_channels[-1] * img_size**2
    widths, acts, norms = sample_MLP(
        trial, 
        in_dim, 
        output_dim, 
        "MLP",
        search_space=spaces,  # Pass search space
        num_layers=3
    )
    mlp = MLP(widths, acts, norms)

    bops += get_MLP_bops(mlp, bit_width=32)

    # Initialize Model
    Blocks = nn.Sequential(*Blocks)
    model = CandidateArchitecture(Blocks, mlp, block_channels[0])
    bops += get_conv2d_bops(
        model.conv, 
        input_shape=[batch_size, 1, 11, 11], 
        bit_width=32
    )

    print(model)
    print("BOPs:", bops)
    print("Trial ", trial.number, " begins evaluation...")
    mean_distance, inference_time, validation_loss, param_count = evaluate_BraggNN(model, train_loader, val_loader, device)
    
    with open("./global_search.txt", "a") as file:
        file.write(
            f"Trial {trial.number}, Mean Distance: {mean_distance}, BOPs: {bops}, "
            f"Inference time: {inference_time}, Validation Loss: {validation_loss}, "
            f"Param Count: {param_count}, Hyperparams: {trial.params}\n"
        )
    return mean_distance, bops


def Deepsets_objective(trial):
    """DeepSets objective using search space config"""
    task_configs, search_space = load_configs(task="deepsets")
    spaces = search_space["search_spaces"]
    hyper_params = search_space["hyperparameters"]
    
    bops = 0
    in_dim, out_dim = 3, 5

    # Sample architecture parameters
    bottleneck_dim = 2 ** trial.suggest_int("bottleneck_dim", 
                                           *spaces["bottleneck_range"])

    aggregator_type = trial.suggest_categorical("aggregator_type", 
                                              spaces["aggregator_space"])
    aggregator = (lambda x: torch.mean(x, dim=2) if aggregator_type == "mean" 
                 else lambda x: torch.max(x, dim=2).values)
    
    if aggregator_type == "mean":
        bops += get_AvgPool_bops(input_shape=(8, bottleneck_dim), bit_width=8)
    else:
        bops += get_MaxPool_bops(input_shape=(8, bottleneck_dim), bit_width=8)

    # Initialize networks
    phi_len = trial.suggest_int("phi_len", *hyper_params["phi_len_range"])
    phi_widths, phi_acts, phi_norms = sample_MLP(
        trial, 
        in_dim, 
        bottleneck_dim, 
        "phi_MLP", 
        search_space=spaces,
        num_layers=phi_len
    )
    phi = ConvPhi(phi_widths, phi_acts, phi_norms)
    bops += get_MLP_bops(phi, bit_width=8)

    rho_len = trial.suggest_int("rho_len", *hyper_params["rho_len_range"])
    rho_widths, rho_acts, rho_norms = sample_MLP(
        trial, 
        bottleneck_dim, 
        out_dim, 
        "rho_MLP", 
        search_space=spaces,
        num_layers=rho_len
    )
    rho = Rho(rho_widths, rho_acts, rho_norms)
    bops += get_MLP_bops(rho, bit_width=8)

    model = DeepSetsArchitecture(phi, rho, aggregator)

    print(model)
    print("BOPs:", bops)
    print("Trial ", trial.number, " begins evaluation...")
    
    metrics = evaluate_deepsets(model, train_loader, val_loader, test_loader, device)
    
    accuracy = metrics['val_accuracy']
    inference_time = metrics['inference_time']
    validation_loss = metrics['val_loss']
    param_count = metrics['param_count']

    with open("./global_search.txt", "a") as file:
        file.write(
            f"Trial {trial.number}, Accuracy: {accuracy}, BOPs: {bops}, "
            f"Inference time: {inference_time}, Validation Loss: {validation_loss}, "
            f"Param Count: {param_count}, Hyperparams: {trial.params}\n"
        )
    return accuracy, bops

if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    batch_size = 4096
    num_workers = 4

    # BraggNN optimization
    if False:  # Change this flag to switch between tasks
        # BraggNN data setup
        braggnn_configs, braggnn_search_space = load_configs(task="braggnn")
        
        train_loader, val_loader, test_loader = setup_data_loaders_braggnn(
            batch_size, IMG_SIZE=11, aug=1, num_workers=4, 
            pin_memory=False, prefetch_factor=2, 
            data_folder="/home/users/ddemler/dima_stuff/Morph/data/"
        )
        
        study = optuna.create_study(
            sampler=optuna.samplers.NSGAIISampler(population_size=20),
            directions=['minimize', 'minimize']
        )

        # Queue example architectures from config
        study.enqueue_trial(braggnn_configs['openhls'])
        study.enqueue_trial(braggnn_configs['braggnn'])
        
        study.optimize(BraggNN_objective, n_trials=5)
        
    else:
        # Deepsets optimization
        deepsets_configs, deepsets_search_space = load_configs(task="deepsets")

        base_file_name = "jet_images_c8_minpt2_ptetaphi_robust_fast"
        
        train_loader, val_loader, test_loader = setup_data_loaders_deepsets(
            base_file_name,
            batch_size=batch_size,
            num_workers=num_workers,
            prefetch_factor=2,
            pin_memory=True
        )
        
        study = optuna.create_study(
            sampler=optuna.samplers.NSGAIISampler(population_size=20),
            directions=["maximize", "minimize"]
        )

        # Queue example architectures from config
        study.enqueue_trial(deepsets_configs['base'])
        study.enqueue_trial(deepsets_configs['large'])
        study.enqueue_trial(deepsets_configs['medium'])
        study.enqueue_trial(deepsets_configs['small'])
        study.enqueue_trial(deepsets_configs['tiny'])

        study.optimize(Deepsets_objective, n_trials=1000)

================
File: local_search.py
================
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
from typing import List, Tuple, Union, Callable
from dataclasses import dataclass
from torch.utils.data import DataLoader
import brevitas.nn as qnn


from models.blocks import *


@dataclass
class SearchConfig:
    """Configuration for local search parameters"""
    num_prune_iterations: int = 20
    prune_amount: float = 0.2
    include_bias: bool = False
    log_file: str = "local_search_results.txt"
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

class LocalSearch:
    def __init__(self, config: SearchConfig):
        self.config = config
        
    @staticmethod
    def get_parameters_to_prune(model: nn.Module, bias: bool = False) -> tuple:
        """Get all parameters that can be pruned from the model"""
        parameters_to_prune = []
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear, qnn.QuantLinear, qnn.QuantConv1d, qnn.QuantConv2d)):
                parameters_to_prune.append((module, "weight"))
                if bias and module.bias is not None:
                    parameters_to_prune.append((module, "bias"))
        return tuple(parameters_to_prune)

    @staticmethod
    def get_sparsities(model: nn.Module) -> tuple:
        """Calculate sparsity for each layer in the model"""
        sparsities = []
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear, qnn.QuantLinear, qnn.QuantConv1d, qnn.QuantConv2d)):
                layer_sparsity = torch.sum(module.weight_mask == 0).float() / module.weight_mask.numel()
                sparsities.append(layer_sparsity)
        return tuple(sparsities)

    def search_single_model(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: DataLoader,
        evaluate_fn: Callable,
        model_name: str = "Model",
        extra_info: str = ""
    ) -> None:
        """
        Perform local search on a single model
        
        Args:
            model: The model to search
            train_loader: Training data loader
            val_loader: Validation data loader
            test_loader: Test data loader
            evaluate_fn: Function to train and evaluate the model
            model_name: Name identifier for the model
            extra_info: Additional information to log
        """
        model = model.to(self.config.device)
        # Initialize pruning
        prune.global_unstructured(
            self.get_parameters_to_prune(model, self.config.include_bias),
            pruning_method=prune.L1Unstructured,
            amount=0
        )

        for prune_iter in range(self.config.num_prune_iterations):
            
            # Train and evaluate model
            metrics = evaluate_fn(model, train_loader, val_loader, test_loader, self.config.device)
            sparsities = self.get_sparsities(model)

            print(f"Pruning Iter {prune_iter + 1}/{self.config.num_prune_iterations}")
            
            # Log results
            with open(self.config.log_file, "a") as file:
                log_str = f"{model_name} {extra_info} Prune Iter: {prune_iter}, "
                log_str += f"Metrics: {metrics}, Sparsities: {sparsities}\n"
                file.write(log_str)

            # Apply pruning
            if prune_iter < self.config.num_prune_iterations - 1:  # Don't prune on last iteration
                prune.global_unstructured(
                    self.get_parameters_to_prune(model, self.config.include_bias),
                    pruning_method=prune.L1Unstructured,
                    amount=self.config.prune_amount
                )

    def search_multiple_models(
        self,
        models: List[Tuple[nn.Module, str]],
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: DataLoader,
        evaluate_fn: Callable,
        extra_info: str = ""
    ) -> None:
        """
        Perform local search on multiple models
        
        Args:
            models: List of (model, model_name) tuples
            train_loader: Training data loader
            val_loader: Validation data loader
            test_loader: Test data loader
            evaluate_fn: Function to evaluate model performance
            extra_info: Additional information to log
        """
        for model, model_name in models:
            print(f"Searching {model_name}...")
            self.search_single_model(
                model=model,
                train_loader=train_loader,
                val_loader=val_loader,
                test_loader=test_loader,
                evaluate_fn=evaluate_fn,
                model_name=model_name,
                extra_info=extra_info
            )




# Example usage for DeepSets
from utils.processor import evaluate_deepsets
from data.DeepsetsDataset import *
if __name__ == "__main__":
    # DeepSets Dataset Configuration
    batch_size = 4096
    num_workers = 8
    base_file_name = "jet_images_c8_minpt2_ptetaphi_robust_fast"

    # Load datasets
    train_loader, val_loader, test_loader = setup_data_loaders_deepsets(
        base_file_name,
        batch_size=batch_size,
        num_workers=num_workers,
        prefetch_factor=2,
        pin_memory=True
    )
    print("Loaded Dataset...")

    # Search Configuration
    config = SearchConfig(
        num_prune_iterations=20,
        prune_amount=0.2,
        include_bias=False,
        log_file="Results/deepsets_search_results.txt",
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    
    # Initialize search
    local_search = LocalSearch(config)

    # Define models (using QAT models as example)
    bit_width = 32
    aggregator = lambda x: torch.mean(x, dim=2)

    # Large model
    large_phi = QAT_ConvPhi(
        widths=[3, 32, 32], 
        acts=[nn.ReLU(), nn.ReLU()], 
        norms=["batch", "batch"], 
        bit_width=bit_width
    )
    large_rho = QAT_Rho(
        widths=[32, 32, 64, 5],
        acts=[nn.ReLU(), nn.ReLU(), nn.LeakyReLU(negative_slope=0.01)],
        norms=["batch", None, "batch"],
        bit_width=bit_width
    )
    large_model = DeepSetsArchitecture(large_phi, large_rho, aggregator)

    # Small model
    small_phi = QAT_ConvPhi(
        widths=[3, 8, 8], 
        acts=[nn.LeakyReLU(negative_slope=0.01), nn.ReLU()], 
        norms=["batch", None], 
        bit_width=bit_width
    )
    small_rho = QAT_Rho(
        widths=[8, 16, 16, 5],
        acts=[nn.LeakyReLU(negative_slope=0.01), nn.ReLU(), nn.LeakyReLU(negative_slope=0.01)],
        norms=["batch", "batch", None],
        bit_width=bit_width
    )
    small_model = DeepSetsArchitecture(small_phi, small_rho, aggregator)

    # Define models to search
    deepsets_models = [
        (large_model, "Large"),
        (small_model, "Small")
    ]

    # Run search on multiple models
    local_search.search_multiple_models(
        models=deepsets_models,
        train_loader=train_loader,
        val_loader=val_loader,
        test_loader=test_loader,
        evaluate_fn=evaluate_deepsets,
        extra_info=f"{bit_width}-Bit QAT"
    )


#Example Usage for BraggNN
"""
from data.BraggnnDataset import *
from utils.processor import evaluate_braggnn
from data.BraggnnDataset import setup_data_loaders

if __name__ == "__main__":
    # BraggNN Dataset Configuration
    batch_size = 4096
    num_workers = 4
    device = "cuda" if torch.cuda.is_available() else "cpu"

    

    # Load datasets
    train_loader, val_loader, test_loader = setup_data_loaders(
        batch_size, IMG_SIZE=11, aug=1, num_workers=4, pin_memory=False, prefetch_factor=2, data_folder= "/home/users/ddemler/dima_stuff/Morph/data/"
    )
    print("Loaded Dataset...")

    config = SearchConfig(
        num_prune_iterations=20,
        prune_amount=0.2,
        include_bias=False,
        log_file="Results/bragg_search_results.txt",
        device=device, 
    )

    # Initialize search
    local_search = LocalSearch(config)

    # NAC Model
    b = 8  # Bit width
    Blocks = nn.Sequential(
        QAT_ConvBlock(
            [32, 4, 32], [1, 1], [nn.ReLU(), nn.LeakyReLU(negative_slope=0.01)], [None, "batch"], img_size=9, bit_width=b
        ),
        QAT_ConvBlock([32, 4, 32], [1, 3], [nn.GELU(), nn.GELU()], ["batch", "layer"], img_size=9, bit_width=b),
        QAT_ConvBlock([32, 8, 64], [3, 3], [nn.GELU(), None], ["layer", None], img_size=7, bit_width=b),
    )

    mlp = QAT_MLP(
        widths=[576, 8, 4, 4, 2],
        acts=[nn.ReLU(), nn.GELU(), nn.GELU(), None],
        norms=["layer", None, "layer", None],
        bit_width=b,
    )

    braggnn_model = QAT_CandidateArchitecture(Blocks, mlp, 32).to(device)

    #initialize pruning
    local_search.search_single_model(
        model=braggnn_model,
        train_loader=train_loader,
        val_loader=val_loader,
        test_loader=test_loader,
        evaluate_fn=evaluate_braggnn,
        model_name="BraggNN",
        extra_info=f"{b}-Bit QAT"
    )
"""

================
File: README.md
================
# Neural Architecture Codesign for Fast Physics Applications
This repository contains the implementation of Neural Architecture Codesign (NAC), a framework for optimizing neural network architectures for physics applications with hardware efficiency in mind. NAC employs a two-stage optimization process to discover models that balance task performance with hardware constraints.
## Overview

NAC automates the design of deep learning models for physics applications while considering hardware constraints. The framework uses neural architecture search and network compression in a two-stage approach:

1. Global Search Stage: Explores diverse architectures while considering hardware constraints
2. Local Search Stage: Fine-tunes and compresses promising candidates
3. FPGA Synthesis (*optional*): Converts optimized models to FPGA-deployable code

The framework is demonstrated through two case studies:
- BraggNN: Fast X-ray Bragg peak analysis for materials science
- Jet Classification: Deep Sets architecture for particle physics

## Installation

1. Create a conda environment:
```bash
conda create --name NAC_env python=3.10.10
conda activate NAC_env
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Download datasets:
- For BraggNN:
```bash
python data/get_dataset.py
```
- For Deep Sets: Download `normalized_data3.zip` and extract to `/data/normalized_data3/`

## Usage

### Global Search

Run architecture search for either BraggNN or Deep Sets:

```bash
python global_search.py
```

The script will output results to `global_search.txt`. For the Deep Sets model, results will be in `Results/global_search.txt`.

### Local Search

Run model compression and optimization:

```bash
python local_search.py
```

Results will be saved in `Results/deepsets_search_results.txt` or `Results/bragg_search_results.txt`.

## Directory Structure

```
.
├── data/
│   ├── BraggnnDataset.py
│   ├── DeepsetsDataset.py
│   └── get_dataset.py
├── examples/
│   ├── BraggNN/
│   └── DeepSets/
├── models/
│   └── blocks.py
├── utils/
│   ├── bops.py
│   ├── metrics.py
│   └── processor.py
├── global_search.py
├── local_search.py
└── requirements.txt
```

================
File: requirements.txt
================
torch==1.13.0
numpy==1.24.4
h5py==3.10.0
scikit-image==0.21.0
optuna==3.4.0
brevitas==0.9.1
